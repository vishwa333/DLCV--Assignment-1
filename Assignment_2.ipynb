{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "\n",
    "In this assignment we will cover topics from the previous 3 lectures. We will cover the following topics:\n",
    "\n",
    "1) Training a simple Linear Model\n",
    "\n",
    "2) Implementing Modules with Backprop functionality\n",
    "\n",
    "3) Implementing Convolution Module on Numpy.\n",
    "\n",
    "4) Implement Dropout/Different Optimizer setups.\n",
    "\n",
    "5) Implementing Pool and Training on CIFAR10?\n",
    "\n",
    "\n",
    "It is crucial to get down to the nitty gritty of the code to implement all of these. No external packages (like caffe,pytorch etc), which directly give functions for these steps, are to be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple Linear Model\n",
    "\n",
    "In this section, you will write the code to train a Linear Model. The goal is to classify and input $x_n$ of size $n$ into one of $m$ classes. For this goal, you need to create the following parts:\n",
    "\n",
    "1) ** A weight Matrix $W_{n\\times m}$ **, where the Weights are multipled to the input $X_n$ (Vector of size $n$), to find $m$ scores $S_m$ for the $m$ classes.\n",
    "\n",
    "2) ** The Loss function **: We learnt two Kinds of Loss functions:\n",
    "  *  The Hinge Loss: This loss measures, for each sample, how many times were the wrong classes scored above correct class score - $\\Delta$ ? and by how much? This leads to the formulation:\n",
    "  \n",
    "$$\n",
    "L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)\n",
    "$$\n",
    "\n",
    "where $y_i$ is the correct class, and $s_j$ is the score for the $j$-th class (the $j$-th element of $S_m$)\n",
    "  \n",
    "  * The softmax Loss: By interpreting the scores as unnormalized log probabilities for each class, this loss tries to measure dissatisfaction with the scores in terms of the log probability of the right class:\n",
    "\n",
    "$$\n",
    "L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}\n",
    "$$\n",
    "\n",
    "where $f_{ y_i }$ is the $i$-th element of the output of $W^T_{n \\times m} . X_m$\n",
    "\n",
    "4) ** Regularization term **: In addition to the loss, you need a Regularization term to lead to a more distributed( in case of $L_2$) or sparse (in case of $L_1$) learning of the weights. For example, and having $L_2$ regularization would imply that your loss has the following additional term:\n",
    "\n",
    "$$\n",
    "R(W) = \\sum_k\\sum_l W_{k,l}^2,\n",
    "$$\n",
    "\n",
    "making the total loss:\n",
    "$$\n",
    "L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss} \\\\\\\\\n",
    "$$\n",
    "\n",
    "3) ** An Optimization Procedure **: This refers to the process which tweaks the weight Matrix $W_{n\\times m}$ to reduce the loss function $L$. In our case, this refers to Mini-batch Gradient Descent algorithm. We adjust the weights $W_{n\\times m}$, based on the gradient of the loss $L$ w.r.t. $W_{n\\times m}$. This leads to:\n",
    "\n",
    "$$\n",
    "W_{t+1} = W_{t} + \\alpha \\frac{\\partial L}{\\partial W},\n",
    "$$\n",
    "where $\\alpha$ is the learning rate. Additionally, as we will be doing \"mini-batch\" gradient Descent, instead of finding loss over the whole dataset, we find it only for a small sample of the traning data for each learning step we take. Basically,\n",
    "$$\n",
    "W_{t+1} = W_{t} + \\alpha \\frac{\\partial \\sum^{b}{L_{x_i}}}{\\partial W},\n",
    "$$\n",
    "where, $b$ is the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Train a **Single-Layer Classifier** for the MNIST dataset. Guidelines:\n",
    "* Use a loss of your choice.\n",
    "* Keep a validation split of the trainingset for finding the right value of $\\lambda$ for the regularization, and to check for over fitting.\n",
    "* Finally,evaluate the classification performance on the testset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load The Mnist data:\n",
    "# Download data from http://yann.lecun.com/exdb/mnist/\n",
    "# load the data.\n",
    "\n",
    "# split the data into train, and valid\n",
    "\n",
    "# Now a function, which returns a generator random mini-batch of the input data\n",
    "\n",
    "def get_minibatch_function(training_x, training_y):\n",
    "    \n",
    "    def get_minibatch(training_x=training_x, training_y=training_y):\n",
    "        ## Read generator functions if required.\n",
    "\n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        yield mini_x,mini_y\n",
    "        \n",
    "    return get_minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the class Single Layer Classifier\n",
    "class single_layer_classifier():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        # Give the instance a weight matrix, initialized randomly.\n",
    "        \n",
    "    # Define the forward function\n",
    "    def forward(self, input_x):\n",
    "        \n",
    "        # get the scores\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    # Similarly a backward function\n",
    "    # we define 2 backward functions (as Loss = L1 + L2, grad(Loss) = grad(L1) + grad(L2))\n",
    "    \n",
    "    def backward_from_loss(self, grad_from_loss):\n",
    "        \n",
    "        # this function returns a matrix of the same size as the weights, \n",
    "        # where each element is the partial derivative of the loss w.r.t. the respective element of weight.\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return grad_matrix\n",
    "        \n",
    "    def backward_from_l2(self):\n",
    "        \n",
    "        # this function returns a matrix of the same size as the weights, \n",
    "        # where each element is the partial derivative of the regularization_term\n",
    "        # w.r.t. the respective element of weight.\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return grad_matrix\n",
    "    \n",
    "    # BONUS\n",
    "    def grad_checker(input_x, grad_matrix):\n",
    "        \n",
    "        # Guess what to do?\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        if diff<threshold:\n",
    "            return true\n",
    "        else:\n",
    "            return false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we need the loss functions,one which calculates the loss, \n",
    "# and one which give the backward gradient\n",
    "# Make any one of the suggested losses\n",
    "\n",
    "def loss_forward(input_y,scores):\n",
    "\n",
    "    ## WRITE CODE HERE    \n",
    "    \n",
    "    return loss\n",
    "def loss_backward(loss):\n",
    "    # This part deals with the gradient from the loss to the weight matrix.\n",
    "    # for example, in case of softmax loss(-log(qc)), this part gives grad(loss) w.r.t. qc\n",
    "\n",
    "    ## WRITE CODE HERE    \n",
    "\n",
    "    return grad_from_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally the trainer:\n",
    "\n",
    "# let it be for t iterations:\n",
    "\n",
    "# make an instance of single_layer_classifier,\n",
    "# get the mini-batch yielder.\n",
    "\n",
    "for iter,input_x, input_y in enumerate(get_minibatch()):\n",
    "    \n",
    "    ## Write code here for each iteration of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the performance on the validation set.\n",
    "# find the top-1 accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now make a trainer function based on the above code, which trains for 't' iteration,\n",
    "# and returns the performance on the validation\n",
    "\n",
    "def trainer(iterations, kwargs):\n",
    "\n",
    "    ## WRITE CODE HERE\n",
    "    \n",
    "    return top_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the optimal lambda and iterations t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train on whole dataset with these values,(from scratch)\n",
    "# report final performance on mnist test set.\n",
    "\n",
    "# Find the best performing class and the worst performing class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Backprop\n",
    "\n",
    "Now that you have had some experience with single layer networks, its time to go to more complex architectures. But first we need to completely understand and implement backpropagation.\n",
    "\n",
    "## Backpropagation:\n",
    "\n",
    "Simple put, a way of computing gradients of expressions through recursive application of chain rule. If,\n",
    "$$\n",
    "L = f (g (h (\\textbf{x})))\n",
    "$$\n",
    "then,\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\textbf{x}} = \\frac{\\partial f}{\\partial g} \\times \\frac{\\partial g}{\\partial h} \\times\\frac{\\partial h}{\\partial \\textbf{x}} \n",
    "$$\n",
    "\n",
    "** Look into the class Lecture for more detail **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Scalar Backpropagation\n",
    "\n",
    "Evaluate the gradient of the following functions w.r.t. the input.\n",
    "\n",
    "1) $$ f(x,y,z) =  log(\\sigma(\\frac{cos(\\pi \\times \\sigma(x))+sin(\\pi \\times \\sigma(y/2))}{z^2}))$$\n",
    "where $\\sigma$ is the sigmoid function. Find gradient for the following values:\n",
    "  * $(x,y,z)$ =  (1,2,3)\n",
    "  * $(x,y,z)$ =  (3,2,1)\n",
    "  * $(x,y,z)$ =  (12,23,31)\n",
    "  * $(x,y,z)$ =  (32,21,13)\n",
    "\n",
    "2) $$ f(x,y,z) = -tan(z) + exp(4x^2 + 3x + 10) - x^{y^z} $$\n",
    "where $\\exp$ is the exponential function. Find gradient for the following values:\n",
    "  * $(x,y,z)$ =  (-0.1 ,2 ,-3)\n",
    "  * $(x,y,z)$ =  (-3, 0.2,0.5)\n",
    "  * $(x,y,z)$ =  (1.2, -2.3, 3.1)\n",
    "  * $(x,y,z)$ =  (3.2, 2.1, -1.3)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To solve this problem, construct the computational graph (will help understanding the problem)(not part of assignment)\n",
    "# Write each component of the graph as a class, with forward and backward functions.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class sigmoid():\n",
    "    def __init__(self):\n",
    "        self.forw = 0\n",
    "        self. back = 0\n",
    "        self.ip=0\n",
    "    def forward(self,x):\n",
    "        # save values useful for backpropagation\n",
    "        self.ip=x\n",
    "        self.forw = 1/(1+np.exp(-x))\n",
    "        return self.forw\n",
    "    def backward(self,grad):\n",
    "    \tself.back = self.forw*(1-self.forw)*grad\n",
    "    \treturn self.back\n",
    "\n",
    "class f_log():\n",
    "\tdef __init__(self):\n",
    "\t\tself.forw=0\n",
    "\t\tself.back=0\n",
    "\tdef forward(self,x):\n",
    "\t\tself.forw = np.log(x)\n",
    "\t\treturn self.forw\n",
    "\tdef backward(self,grad):\n",
    "\t\tself.back = (1/np.exp(self.forw))*grad\n",
    "\t\treturn self.back\n",
    "\n",
    "\n",
    "class h_func():\n",
    "\tdef __init__(self):\n",
    "\t\tself.ip = np.array([])\n",
    "\t\tself.forw=0\n",
    "\t\tself.back= np.array([])\n",
    "\tdef forward(self,x,y,z):\n",
    "\t\tself.ip=np.append(self.ip,x)\n",
    "\t\tself.ip=np.append(self.ip,y)\n",
    "\t\tself.ip=np.append(self.ip,z)\n",
    "\t\tself.forw = (x+y)/(z*z)\n",
    "\t\treturn self.forw\n",
    "\tdef backward(self,grad):\n",
    "\t\tt1 = 1/(self.ip[2]*self.ip[2])\n",
    "\t\tt2 = t1/self.ip[2]\n",
    "\t\tt2=t2*-1*(self.ip[0]+self.ip[1])\n",
    "\t\tself.back = np.append(self.back,t1*grad)\n",
    "\t\tself.back = np.append(self.back,t1*grad)\n",
    "\t\tself.back = np.append(self.back,t2*grad)\n",
    "\t\treturn self.back\n",
    "\n",
    "\n",
    "class cos_func():\n",
    "\tdef __init__(self):\n",
    "\t\tself.forw=0\n",
    "\t\tself.back=0\n",
    "\t\tself.ip=0\n",
    "\tdef forward(self,x):\n",
    "\t\tself.ip=x\n",
    "\t\tself.forw = np.cos(np.pi*x)\n",
    "\t\treturn self.forw\n",
    "\tdef backward(self,grad):\n",
    "\t\tback= np.pi*np.sin(np.pi*self.ip)*-1\n",
    "\t\tself.back = back*grad\n",
    "\t\treturn self.back\n",
    "\n",
    "class sin_func():\n",
    "\tdef __init__(self):\n",
    "\t\tself.forw=0\n",
    "\t\tself.back=0\n",
    "\t\tself.ip=0\n",
    "\tdef forward(self,x):\n",
    "\t\tself.ip=x\n",
    "\t\tself.forw = np.sin(np.pi*x)\n",
    "\t\treturn self.forw\n",
    "\tdef backward(self,grad):\n",
    "\t\tback= np.pi*np.cos(np.pi*self.ip)\n",
    "\t\tself.back = back*grad\n",
    "\t\treturn self.back\n",
    "\n",
    "\n",
    "\n",
    "class exp_func():\n",
    "    def __init__(self):\n",
    "        self.forw = 0\n",
    "        self. back = 0\n",
    "        self.ip=0\n",
    "    def forward(self,x):\n",
    "        # save values useful for backpropagation\n",
    "        self.ip=x\n",
    "        self.forw = np.exp(4*x*x+3*x+10)\n",
    "        return self.forw\n",
    "    def backward(self,grad):\n",
    "    \tx= self.ip\n",
    "    \tself.back = np.exp(4*x*x+3*x+10)*(8*x+3)*grad\n",
    "    \treturn self.back\n",
    "\n",
    "class power_func():\n",
    "\tdef __init__(self):\n",
    "\t\tself.forw=0\n",
    "\t\tself.back= np.array([])\n",
    "\t\tself.ip = np.array([])\n",
    "\tdef forward(self,x,y,z):\n",
    "\t\tself.ip = np.append(self.ip,x)\n",
    "\t\tself.ip = np.append(self.ip,y)\n",
    "\t\tself.ip = np.append(self.ip,z)\n",
    "\t\tself.forw = x**(y**z)\n",
    "\t\treturn self.forw\n",
    "\tdef backward(self,grad):\n",
    "\t\tx=self.ip[0]\n",
    "\t\ty=self.ip[1]\n",
    "\t\tz=self.ip[2]\n",
    "\n",
    "\t\ttmp1 = y**z\n",
    "\t\ttmp2 = x**tmp1\n",
    "\t\ttox = tmp1*(tmp2/x)\n",
    "\t\ttoy = (tmp2)*(tmp1/y)*np.log(z)\n",
    "\t\ttoz = (tmp2)*(tmp1)*np.log(y)*np.log(x)\n",
    "\t\tself.back =np.append(self.back,[tox*grad,toy*grad,toz*grad])\n",
    "\t\treturn self.back\n",
    "\n",
    "\n",
    "class tan_func():\n",
    "\tdef __init__(self):\n",
    "\t\tself.forw=0\n",
    "\t\tself.back=0\n",
    "\t\tself.ip=0\n",
    "\tdef forward(self,z):\n",
    "\t\tself.ip=z\n",
    "\t\tself.forw = np.sin(z)/np.cos(z)\n",
    "\t\treturn self.forw\n",
    "\tdef backward(self,grad):\n",
    "\t\ttmp = np.cos(self.ip)\n",
    "\t\tback= 1/(tmp*tmp)\n",
    "\t\tself.back = back*grad\n",
    "\t\treturn self.back\n",
    "\n",
    "# CAUTION: Carefully treat the input and output dimension variation. At worst, handle them with if statements.\n",
    "# Similarly create the classes for various sub-parts/elements of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write func_1_creator, \n",
    "# which constructs the graph(all operators), forward and backward functions.\n",
    "\n",
    "def calc_grad(a):\n",
    "\tx = a[0]\n",
    "\ty = a[1]\n",
    "\tz = a[2]\n",
    "\tsi1 = sigmoid()\n",
    "\tsi2 = sigmoid()\n",
    "\tsi3 = sigmoid()\n",
    "\tsn1 = sin_func()\n",
    "\tc1 = cos_func()\n",
    "\th = h_func()\n",
    "\tl1 = f_log()\n",
    "\tgrad = np.array([])\n",
    "\n",
    "\tx1 = si1.forward(x)\n",
    "\tx2 = c1.forward(x1)\n",
    "\ty1 = si1.forward(y/2)\n",
    "\ty2 = sn1.forward(y1)\n",
    "\ttmp1 = h.forward(x2,y2,z)\n",
    "\ttmp2 = si3.forward(tmp1)\n",
    "\ttmp3 = l1.forward(tmp2)\n",
    "\n",
    "\tgrad1 = l1.backward(1)\n",
    "\tgrad2 = si3.backward(grad1)\n",
    "\tgrad_grp = h.backward(grad2)\n",
    "\tgrad6 = c1.backward(grad_grp[0])\n",
    "\tgrad7 = sn1.backward(grad_grp[1]) \n",
    "\tgrad8 = si1.backward(grad6)\n",
    "\tgrad9 = si2.backward(grad7)\n",
    "\n",
    "\tgrad = np.append(grad,grad8)\n",
    "\tgrad = np.append(grad,grad9)\n",
    "\tgrad = np.append(grad,grad_grp[2])\n",
    "\treturn grad\n",
    "\n",
    "    \n",
    "    \n",
    "def calc_grad2(a):\n",
    "\tx = a[0]\n",
    "\ty = a[1]\n",
    "\tz = a[2]\n",
    "\tf1 = power_func()\n",
    "\tf2 = exp_func()\n",
    "\tf3 = tan_func()\n",
    "\tout = -f3.forward(z)+f2.forward(x)-f1.forward(x,y,z)\n",
    "\tgrad1 = f1.backward(1)\n",
    "\tgrad2 = f2.backward(-1)\n",
    "\tgrad3 = f3.backward(-1)\n",
    "\tgrad=np.array([])\n",
    "\tgrad = np.append(grad,[grad2+grad1[0],grad1[1],grad1[2]+grad3])\n",
    "\treturn grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradients for question a are as follows\n",
      "[[ -2.55447254e-02  -0.00000000e+00  -1.55022009e-03]\n",
      " [ -5.13424495e-02  -0.00000000e+00   1.34958702e-01]\n",
      " [ -5.27208000e-13  -0.00000000e+00   1.67914432e-05]\n",
      " [ -1.69657528e-20  -0.00000000e+00   2.28223738e-04]]\n",
      "The gradients for question b are as follows\n",
      "[[ -2.65704472e+08   4.39444915e+00  -1.02031952e+00]\n",
      " [ -2.07759202e+25   0.00000000e+00   1.02814814e+01]\n",
      " [             inf              inf              inf]\n",
      " [             nan              inf              inf]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:115: RuntimeWarning: overflow encountered in double_scalars\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:91: RuntimeWarning: overflow encountered in exp\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:95: RuntimeWarning: overflow encountered in exp\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[3,2,1],[12,23,31],[32,21,13]])\n",
    "b = np.array([[-0.1,2,-3],[-3,0.2,0.5],[1.2,-2.3,3.1],[3.2,2.1,-1.3]])\n",
    "\n",
    "#question a\n",
    "len = a[:,0].shape[0]\n",
    "#print len\n",
    "grads=np.array([])\n",
    "grad = calc_grad(a[0])\n",
    "grads=np.append(grads,grad)\n",
    "grads = np.reshape(grads,[1,3])\n",
    "for i in range(1,len):\n",
    "\tgrad = calc_grad(a[i])\n",
    "\tgrad = np.reshape(grad,[1,3])\n",
    "\tgrads=np.append(grads,grad,axis=0)\n",
    "print \"The gradients for question a are as follows\"\n",
    "print grads\n",
    "\n",
    "\n",
    "#question b\n",
    "\n",
    "len = b[:,0].shape[0]\n",
    "#print len\n",
    "grads=np.array([])\n",
    "grad = calc_grad2(a[0])\n",
    "grads=np.append(grads,grad)\n",
    "grads = np.reshape(grads,[1,3])\n",
    "for i in range(1,len):\n",
    "\tgrad = calc_grad2(a[i])\n",
    "\tgrad = np.reshape(grad,[1,3])\n",
    "\tgrads=np.append(grads,grad,axis=0)\n",
    "print \"The gradients for question b are as follows\"\n",
    "print grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Modular Vector Backpropagation\n",
    "\n",
    "* Construct a Linear Layer module, implementing the forward and backward functions for arbitrary sizes.\n",
    "* Construct a ReLU module, implementing the forward and backward functions for arbitrary sizes.\n",
    "* Create a 2 layer MLP using the constructed modules.\n",
    "\n",
    "* Modifying the functions built in Question 1 , train this two layer MLP for the same data set (MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class for Linear Layer (Refer code of pytorch/tensorflow package if required.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class for ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your 2 layer MLP \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validation Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best Class and worst class performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After the lecture on Jan 31st.\n",
    "\n",
    "# Implementing Convolution Module on Numpy.\n",
    "\n",
    "* This topic will require you to implement the Convolution operation using Numpy.\n",
    "* You will implement <s>two</s> one methods of doing it, an intuitive <s>and an optimised</s> way.\n",
    "* <s>Additional operations like dropout, batch norms.</s>\n",
    "* We will use the created Module for interesting task like Blurring, Bilateral Filtering.\n",
    "* Finally, we create the Backprop for this.\n",
    "* <s>Train a Conv model for the same MNIST dataset. (can be a script based training, instead of having it in jupyter notebook.)</s>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "<br>\n",
    "* Implement a naive Convolution module, with basic functionalities:\n",
    "  * kernel_size,padding, stride, dilation\n",
    "  \n",
    "* Test out the convolution layer created, by using it to do gaussian blurring on 10 random images of Cifar10 dataset\n",
    "\n",
    "* Bonus: Bilateral filtering can also be implemented using a 2-D convolution. Try bilateral filter for the space of (X,Y,Gray). (3D space, but not 3D conv), (no speed criteria), (Hint: You have multiple filters in each conv layer.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define a class Convolution Layer, which is initialized with the various required params:\n",
    "Class convolution_layer():\n",
    "    \n",
    "    def __init__(self,kvargs):\n",
    "        ## Refer pytorch documentation/tensorflow documentation for the parameters for the layer.\n",
    "        ## Bonus for implementing Groups, no-bias functionality.\n",
    "        ## Random initialization of the weights\n",
    "        \n",
    "    def forward(self,input):\n",
    "        # Input Proprocess(According to pad etc.) Input will be of size (Batch_size, in_channels, inp_height, inp_width)\n",
    "        \n",
    "        # Reminder: Save Input for backward!\n",
    "        # Simple Conv operation:\n",
    "        # Loop over every location in inp_height * inp_width for the whole batch\n",
    "        \n",
    "        # Output will be of the size (Batch_size, out_channels, out_height, out_width)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_of_output_size):\n",
    "        \n",
    "        # Naive Implementation\n",
    "        # Speed is not a concern\n",
    "        # Hint: gradients from each independant operation can be summed\n",
    "        \n",
    "        #  return gradient of the size of the weight kernel\n",
    "        return grad\n",
    "    \n",
    "    def set_weights(self, new_weights):\n",
    "        ## Replace the set of weights with the given 'new_weights'\n",
    "        ## use this for setting weights for blurring, bilateral filtering etc. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get cifar images\n",
    "\n",
    "## Initialize a conv layer. Set weights for gaussian blurring\n",
    "\n",
    "## generate output.\n",
    "\n",
    "## use matplotlib.pyplot to show the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## BONUS: Bilateral Filter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "<br>\n",
    "Now we will use the created layer for training a simple Convolution Layer. \n",
    "\n",
    "* The goal is to make it learn a set of weights, by using the backpropagation function created. To test the backpropagation, instead of training a whole network, we will train only a single layer.\n",
    "  * Take 100 cifar10 images. Generate a numpy array of size (20,3,5,5), with samples from uniform distribution (-1,1).Initialize a Convolution layer with 20 5$\\times$5 kernels(input size 3) and set the generated weights as the layer weights. Save the output of these 100 images from this Convolution layer. \n",
    "  \n",
    "  * Now, initialize a new convolution layer, and use $L_2$ loss between the output of the network and the output generated in the previous step to get the same set of weights as the ones generated in the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First generate the random weight vector\n",
    "\n",
    "## Init a conv layer with these weights\n",
    "\n",
    "## For all images get output. Store in numpy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for part 2 we need to write a small L2 layer\n",
    "class L2_loss():\n",
    "    def ___init__():\n",
    "        # empty\n",
    "    \n",
    "    def forward(self, inp_1,inp_2):\n",
    "        # input is of dimestion(batch,channels,h,w)\n",
    "        # calculate the l2 norm of inp_1 - inp_2 .,\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self,output_grad):\n",
    "        # from the loss, and the input, get the grad at each location of the input.\n",
    "        # The grad is of the shape (batch,channels,h,w)\n",
    "        return grad\n",
    "\n",
    "# Now Init a new conv layer and a L2 loss layer\n",
    "\n",
    "# Train the new conv-layer using the L2 loss to get the earlier set of generated weights.\n",
    "# Use batches.\n",
    "\n",
    "\n",
    "# Print L2 dist between output from new convolution layer and the outputs generated initially.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
