{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "#### Due Date: 24th Jan'18\n",
    "\n",
    "In this assignment we will cover the basics of Machine Learning. We will cover the following topics:\n",
    "\n",
    "1) Linear Regression\n",
    "\n",
    "2) Logistic Regression\n",
    "\n",
    "3) EM Algorithm\n",
    "\n",
    "4) K-means/Hirarchical Clustering.\n",
    "\n",
    "It is crucial to get down to the nitty gritty of the code to implement all of these. No external packages (like scipy), which directly give functions for these algorithms, are to be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Defination: Given a data set ${\\displaystyle \\{y_{i},\\,x_{i1},\\ldots ,x_{ip}\\}_{i=1}^{n}} $ of $n$ statistical units, a linear regression model assumes that the relationship between the dependent variable $y_i$ and the $p$-vector of regressors $x_i$ is linear. This relationship is modeled through a disturbance term or error variable $Îµ_i$ - an unobserved random variable that adds noise to the linear relationship between the dependent variable and regressors. Thus the model takes the form:\n",
    "\n",
    "$$ {\\displaystyle \\mathbf {y} =X{\\boldsymbol {\\beta }}+{\\boldsymbol {\\varepsilon }},\\,} $$\n",
    "\n",
    "where,\n",
    "\n",
    "$$ \\mathbf {y} ={\\begin{pmatrix}y_{1}\\\\y_{2}\\\\\\vdots \\\\y_{n}\\end{pmatrix}},\\quad $$\n",
    "\n",
    "$$ {\\displaystyle X={\\begin{pmatrix}\\mathbf {x} _{1}^{\\top }\\\\\\mathbf {x} _{2}^{\\top }\\\\\\vdots \\\\\\mathbf {x} _{n}^{\\top }\\end{pmatrix}}={\\begin{pmatrix}1&x_{11}&\\cdots &x_{1p}\\\\1&x_{21}&\\cdots &x_{2p}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\1&x_{n1}&\\cdots &x_{np}\\end{pmatrix}},}\n",
    "$$\n",
    "\n",
    "$$ {\\displaystyle {\\boldsymbol {\\beta }}={\\begin{pmatrix}\\beta _{0}\\\\\\beta _{1}\\\\\\beta _{2}\\\\\\vdots \\\\\\beta _{p}\\end{pmatrix}},\\quad {\\boldsymbol {\\varepsilon }}={\\begin{pmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\vdots \\\\\\varepsilon _{n}\\end{pmatrix}}.} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, in the class lecture we covered the Least Square Solution, which can be formulated as:\n",
    "\n",
    "$${\\displaystyle {\\hat {\\boldsymbol {\\beta }}}=(\\mathbf {X} ^{\\top }\\mathbf {X} )^{-1}\\mathbf {X} ^{\\top }\\mathbf {y} =\\left(\\sum \\mathbf {x} _{i}\\mathbf {x} _{i}^{\\top }\\right)^{-1}\\left(\\sum \\mathbf {x} _{i}y_{i}\\right).} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "a) You will write the code to find the LSS for the given data. The data contains 3 columns, each for $y, X_{1}$, and $X_{2}$ respectively. Few of the possible models are:\n",
    "\n",
    "$$ {\\displaystyle \\mathbf {y} ={\\boldsymbol {\\beta_{1} }}X_1+{\\boldsymbol {\\beta_{0} }}+{\\boldsymbol {\\varepsilon }},\\,} $$\n",
    "\n",
    "\n",
    "$$ {\\displaystyle \\mathbf {y} ={\\boldsymbol {\\beta_{2} }}X_2+{\\boldsymbol {\\beta_{0} }}+{\\boldsymbol {\\varepsilon }},\\,} $$\n",
    "\n",
    "\n",
    "$$ {\\displaystyle \\mathbf {y} ={\\boldsymbol {\\beta_{1} }}X_1+ {\\boldsymbol {\\beta_{2} }}X_2+{\\boldsymbol {\\beta_{0} }}+{\\boldsymbol {\\varepsilon }},\\,} $$\n",
    "\n",
    "Given this data, find the coefficients for each of these models.\n",
    "\n",
    "b) Now that you have three models, you must select the best one. Use Cross-validation with 5 folds on the dataset to find the optimal model (On the basis of RMSE on the test partition). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Load the dataset \n",
    "train_data = np.load('utils/assign_1_data_1_train.npy')\n",
    "# now write the code for finding the solution for each of the three models.\n",
    "rows = train_data.shape[0]\n",
    "cols = train_data.shape[1]\n",
    "\n",
    "y = train_data[:,0]\n",
    "x1 = train_data[:,1]\n",
    "x2 = train_data[:,2]\n",
    "x1=np.asmatrix(x1)\n",
    "x2=np.asmatrix(x2)\n",
    "y= np.asmatrix(y)\n",
    "y=y.transpose()\n",
    "x1=x1.transpose()\n",
    "x2=x2.transpose()\n",
    "m_y, m_x1, m_x2 = np.mean(y), np.mean(x1), np.mean(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, Write The estimates of the betas here:\n",
    "\n",
    "# Model 1\n",
    "x = np.insert(x1,0,1,axis=1)\n",
    "m1beta = np.linalg.inv(x.transpose()*x)* x.transpose()*y\n",
    "error = (y-x*m1beta)\n",
    "residue = error.transpose()*error\n",
    "#print residue.shape\n",
    "#c=residue[0,0]\n",
    "\n",
    "\n",
    "# Model 2\n",
    "\n",
    "x = np.insert(x2,0,1,axis=1)\n",
    "m2beta = np.linalg.inv(x.transpose()*x)* x.transpose()*y\n",
    "error = (y-x*m2beta)\n",
    "residue = error.transpose()*error\n",
    "#print residue.shape\n",
    "#c=residue[0,0]\n",
    "\n",
    "# Model 3\n",
    "x = np.concatenate((x1,x2),axis=1)\n",
    "x = np.insert(x,0,1,axis=1)\n",
    "m3beta = np.linalg.inv(x.transpose()*x)* x.transpose()*y\n",
    "error = (y-x*m3beta)\n",
    "residue = error.transpose()*error\n",
    "#print residue.shape\n",
    "#c=residue[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partition the dataset into 5 random folds.\n",
    "split_x1 = np.split(x1,5)\n",
    "split_x2 = np.split(x2,5)\n",
    "split_y = np.split(y,5)\n",
    "# for each fold, approx. model from the remaining folds, and calculate RMSE on the test fold.\n",
    "m1rms=0\n",
    "m2rms=0\n",
    "m3rms=0\n",
    "index= np.array([[0],[1],[2],[3],[4]])\n",
    "for i in index:\n",
    "\ttmp_x1=np.array([])\n",
    "\ttmp_x2=np.array([])\n",
    "\ttmp_y=np.array([])\n",
    "\ttmp_test_x1=np.array([])\n",
    "\ttmp_test_x2=np.array([])\n",
    "\ttmp_test_y=np.array([])\n",
    "\tprint i[0]\n",
    "\tfor j in index:\n",
    "\t\tif j[0]!=i[0]:\n",
    "\t\t\ttmp_x1 = np.append(tmp_x1,split_x1[j[0]])\n",
    "\t\t\ttmp_x2 = np.append(tmp_x2,split_x2[j[0]])\n",
    "\t\t\ttmp_y = np.append(tmp_y,split_y[j[0]])\n",
    "\t\telse : \n",
    "\t\t\ttmp_test_x1 = np.append(tmp_test_x1,split_x1[j[0]])\n",
    "\t\t\ttmp_test_x2 = np.append(tmp_test_x2,split_x2[j[0]])\n",
    "\t\t\ttmp_test_y = np.append(tmp_test_y,split_y[j[0]]) \t \n",
    "\n",
    "\ttmp_x1=np.asmatrix(tmp_x1)\n",
    "\ttmp_x2=np.asmatrix(tmp_x2)\n",
    "\ttmp_y=np.asmatrix(tmp_y)\n",
    "\ttmp_test_x1=np.asmatrix(tmp_test_x1)\n",
    "\ttmp_test_x2=np.asmatrix(tmp_test_x2)\n",
    "\ttmp_test_y=np.asmatrix(tmp_test_y)\n",
    "\ttmp_x1=tmp_x1.transpose()\n",
    "\ttmp_x2=tmp_x2.transpose()\n",
    "\ttmp_y=tmp_y.transpose()\n",
    "\ttmp_test_x1=tmp_test_x1.transpose()\n",
    "\ttmp_test_x2=tmp_test_x2.transpose()\n",
    "\ttmp_test_y=tmp_test_y.transpose()\n",
    "\t\n",
    "\n",
    "\t#model1\n",
    "\tprint tmp_x1.shape\n",
    "\n",
    "\tx = np.insert(tmp_x1,0,1,axis=1)\n",
    "\tprint x.shape\n",
    "\tprint tmp_y.shape\n",
    "\tbeta = np.linalg.inv(x.transpose()*x)* x.transpose()*tmp_y\n",
    "\tprint beta.shape\n",
    "\ttest_x = np.insert(tmp_test_x1,0,1,axis=1)\n",
    "\terror = (tmp_test_y-test_x*beta)\n",
    "\tresidue = error.transpose()*error\n",
    "\tprint residue.shape\n",
    "\tm1rms = m1rms+residue[0,0]\n",
    "\t#model2\n",
    "\tx = np.insert(tmp_x2,0,1,axis=1)\n",
    "\tbeta = np.linalg.inv(x.transpose()*x)* x.transpose()*tmp_y\n",
    "\terror = (tmp_test_y-test_x*beta)\n",
    "\tresidue = error.transpose()*error\n",
    "\tm2rms = m2rms+residue[0,0]\n",
    "\t#model3\n",
    "\tx = np.concatenate((tmp_x1,tmp_x2),axis=1)\n",
    "\tx = np.insert(x,0,1,axis=1)\n",
    "\tbeta = np.linalg.inv(x.transpose()*x)* x.transpose()*tmp_y\n",
    "\ttmp_test = np.concatenate((tmp_test_x1,tmp_test_x2),axis=1)\n",
    "\ttmp_test = np.insert(tmp_test,0,1,axis=1)\n",
    "\terror = (tmp_test_y-tmp_test*beta)\n",
    "\tresidue = error.transpose()*error\n",
    "\tm3rms = m3rms+residue[0,0]\n",
    "\n",
    "# find avg RMSE for each model. \n",
    "m1rms = m1rms/5\n",
    "m2rms = m2rms/5\n",
    "m3rms = m3rms/5\n",
    "\n",
    "# Which is the best model?\n",
    "if m1rms<=m2rms:\n",
    "\tif m1rms<m3rms:\n",
    "\t\tprint \"I model is best\" \n",
    "\telif m3rms<m2rms:\n",
    "\t\tprint \"III model is best\"\n",
    "elif m2rms<m3rms:\n",
    "\tprint \"II model is best\"\n",
    "else :\n",
    "\tprint \"III model is best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, Give the R^2 score of the best model in the test set:\n",
    "test_data = np.load('utils/assign_1_data_1_test.npy')\n",
    "\n",
    "y = test_data[:,0]\n",
    "x1 = test_data[:,1]\n",
    "x2 = test_data[:,2]\n",
    "x1=np.asmatrix(x1)\n",
    "x2=np.asmatrix(x2)\n",
    "y= np.asmatrix(y)\n",
    "y=y.transpose()\n",
    "x1=x1.transpose()\n",
    "x2=x2.transpose()\n",
    "x = np.concatenate((x1,x2),axis=1)\n",
    "x = np.insert(x,0,1,axis=1)\n",
    "m3beta = np.linalg.inv(x.transpose()*x)* x.transpose()*y\n",
    "error = (y-x*beta)\n",
    "residue = error.transpose()*error\n",
    "c=residue[0,0]\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bonus\n",
    "\n",
    "# Show a graph between the predicted Y^ and the Ground truth Y\n",
    "\n",
    "# Try to plot Y vs X_1 in train set.\n",
    "\n",
    "# can it help you improve your model?\n",
    "# construct the better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Generaly, Logistic Regression is used to predict categorial variables. For the simple problem of 2-way classification, the output $\\hat{y_i}$ is modeled as the probability that $\\{X_i\\}$ belongs to class $1$ (given two classes $0$, and $1$).\n",
    "\n",
    "$$ P( \\{X_i\\} \\in Set_1 ) = \\hat{y_i}, $$ ( $y_i$ is the actual label; $y_i \\in \\{ 0,1 \\}$ )\n",
    "\n",
    "\n",
    "$\\hat{y_i}$ is typically modeled as the output of a sigmoid on a linear combination of the input feature $\\{X_i\\}$:\n",
    "\n",
    "$$ \\mathbf {\\hat{y}} = \\sigma(X{\\boldsymbol {\\beta }}+{\\boldsymbol {\\varepsilon }}) = \\sigma_\\beta(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, The likelihood of some given data for this model can be written as:\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}L(\\beta |x)&=Pr(Y|X;\\beta )\\\\&=\\prod _{i}Pr(y_{i}|x_{i};\\beta )\\\\&=\\prod _{i}\\sigma_{\\beta }(x_{i})^{y_{i}}(1-\\sigma_{\\beta }(x_{i}))^{(1-y_{i})}\\end{aligned}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in the case of Linear regression, this equation has no closed form solution. Hence we will use gradient descent on the negative log-likelihood $J(\\beta)$ to find the optimal $\\beta$\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\sum_i{\\big( y_ilog(\\hat{y_i})+ (1-y_i)log(1-\\hat{y_i}) \\big) }\n",
    "$$\n",
    "\n",
    "with the update equation:\n",
    "\n",
    "$$\n",
    "\\beta_j = \\beta_j + \\alpha \\times \\frac{ \\partial J(\\beta)}{\\partial \\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "a) You will write the code to find the optimal logistic regression model for the given data. The data contains 3 columns, each for $y, X_{1}$, and $X_{2}$ respectively. For the rate of learning $\\alpha$ use a linearly decaying policy, or step-wise reduction policy. \n",
    "\n",
    "$$ {\\displaystyle \\mathbf {y} =\\sigma \\big( {\\boldsymbol {\\beta_{1} }}X_1+ {\\boldsymbol {\\beta_{2} }}X_2+{\\boldsymbol {\\beta_{0} }}+{\\boldsymbol {\\varepsilon }}} \\big) $$\n",
    "\n",
    "b) Explore possible methods of adjusting the learning rate $\\alpha$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cost_func(beta, X, y):\n",
    "   \tlog_func_v = 1.0/(1 + np.exp(-np.dot(X, beta)))\n",
    "\ty = np.squeeze(y)\n",
    "\tstep1 = y * np.log(log_func_v)\n",
    "\t#print log_func_v\n",
    "\tstep2 = (1 - y) * np.log(1 - log_func_v)\n",
    "\tfinal = -step1 - step2\n",
    "\treturn np.mean(final)\n",
    "\n",
    "def normalize(X):\n",
    "    '''\n",
    "    function to normalize feature matrix, X\n",
    "    '''\n",
    "    mins = np.min(X, axis = 0)\n",
    "    maxs = np.max(X, axis = 0)\n",
    "    rng = maxs - mins\n",
    "    norm_X = 1 - ((maxs - X)/rng)\n",
    "    return norm_X\n",
    "\n",
    "def sigmoid_func(beta, X):\n",
    "\tprint beta.shape\n",
    "\tprint X.shape\n",
    "\treturn 1.0/(1 + np.exp(-np.dot(X, beta)))\n",
    "\n",
    "\n",
    "def logistic_grad(beta, X, y):\n",
    "    '''\n",
    "    logistic gradient function\n",
    "    '''\n",
    "    tmp = sigmoid_func(beta, X) - y.reshape(X.shape[0], -1)\n",
    "    tmp1 = np.dot(tmp.T, X)\n",
    "    return tmp1\n",
    "\n",
    "# Load the train dataset \n",
    "train_data = np.load('utils/assign_1_data_2_train.npy')\n",
    "# now write the code to find the parameters of the optimization.\n",
    "y = train_data[:,0]\n",
    "x1 = train_data[:,1]\n",
    "x2 = train_data[:,2]\n",
    "x1=np.asmatrix(x1)\n",
    "x2=np.asmatrix(x2)\n",
    "y= np.asmatrix(y)\n",
    "y=y.transpose()\n",
    "x1=x1.transpose()\n",
    "x2=x2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test on a validation part every 't' iterations to find when you start overfitting.\n",
    "# t = ?\n",
    "x=np.asarray(x2)\n",
    "ply = np.asarray(y)\n",
    "X=np.concatenate((x1,x2),axis=1)\n",
    "print \"x shape is \"\n",
    "print X.shape\n",
    "X=normalize(X)\n",
    "print X\n",
    "X = np.insert(X,0,1,axis=1)\n",
    "print \"x shape after transpose \"\n",
    "print X.shape\n",
    "beta = np.matrix(np.ones(X.shape[1])).transpose()\n",
    "print \"Beta shape is\"\n",
    "print beta.shape\n",
    "\n",
    "converge_change = 0.000001\n",
    "\n",
    "cost = cost_func(beta, X, y)\n",
    "change_cost = 1\n",
    "num_iter = 1\n",
    "lr = 0.0001\n",
    "nlr = 0.0001\n",
    "\n",
    "\n",
    "while (change_cost > converge_change):\n",
    "        old_cost = cost\n",
    "        tmp_clc = sigmoid_func(beta, X) - y.reshape(X.shape[0], -1)\n",
    "    \tlog_grad = np.dot(tmp_clc.T, X)\n",
    "        beta = beta - (nlr * logistic_grad(beta, X, y))\n",
    "        cost = cost_func(beta, X, y)\n",
    "        change_cost = old_cost - cost\n",
    "        nlr=lr/num_iter\n",
    "        num_iter += 1\n",
    "\n",
    "\n",
    "print num_iter\n",
    "print cost\n",
    "# Now for 't' iterations train on the entire dataset for testing on the test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the accuracy on the test set:\n",
    "test_data = np.load('utils/assign_1_data_2_test.npy')\n",
    "y = train_data[:,0]\n",
    "x1 = train_data[:,1]\n",
    "x2 = train_data[:,2]\n",
    "x1=np.asmatrix(x1)\n",
    "x2=np.asmatrix(x2)\n",
    "y= np.asmatrix(y)\n",
    "y=y.transpose()\n",
    "x1=x1.transpose()\n",
    "x2=x2.transpose()\n",
    "\n",
    "x=np.asarray(x2)\n",
    "ply = np.asarray(y)\n",
    "X=np.concatenate((x1,x2),axis=1)\n",
    "X = np.insert(X,0,1,axis=1)\n",
    "\n",
    "pred_prob = sigmoid_func(beta, X)\n",
    "pred_value = np.where(pred_prob >= .5, 1, 0)\n",
    "y_pred = np.squeeze(pred_value)\n",
    "print(\"Correctly predicted labels:\", np.sum(y == y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bonus\n",
    "# Can you adjust the learning rate alpha in a better way?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM algorithm\n",
    "\n",
    "This is a general framework for likelihood-based parameter estimation.\n",
    "A basic outline of this algorithm is:\n",
    "\n",
    "* start with initial guesses of parameters\n",
    "\n",
    "* E step: estimate memberships given params\n",
    "\n",
    "* M step: estimate params given memberships\n",
    "\n",
    "* Repeat until convergence\n",
    "\n",
    "** Refer to [this link](http://www.rmki.kfki.hu/~banmi/elte/bishop_em.pdf) (9.2.2) .**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Let ${\\displaystyle \\mathbf {x} =(\\mathbf {x} _{1},\\mathbf {x} _{2},\\ldots ,\\mathbf {x} _{n})} $ be a sample of $n$ independent observations from a mixture of two multivariate normal distributions of dimension $d$ , and let ${\\displaystyle \\mathbf {z} =(z_{1},z_{2},\\ldots ,z_{n})} $ be the latent variables that determine the component from which the observation originates.\n",
    "\n",
    "$X_i |(Z_i = 1) \\sim \\mathcal{N}_d(\\boldsymbol{\\mu}_1,\\Sigma_1)$ and $X_i |(Z_i = 2) \\sim \\mathcal{N}_d(\\boldsymbol{\\mu}_2,\\Sigma_2)$\n",
    "\n",
    "The aim is to estimate the unknown parameters representing the mixing value between the Gaussians and the means and covariances of each:\n",
    "\n",
    "$$ \\theta = \\big( \\boldsymbol{\\tau},\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\Sigma_1,\\Sigma_2 \\big) $$\n",
    "\n",
    "a) Given the data, find the parameters $\\theta$ using EM algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the train dataset \n",
    "data = np.load('utils/assign_1_data_3.npy')\n",
    "# The data is a 1000*2 numpy array, where each row is a independent observation, and \n",
    "# the columns are measurement in dimension x and y respectively. \n",
    "# now write the code to find the parameter theta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters are given by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the entire data by plotting them as points in a 2-D canvas.  \n",
    "# Show the estimated means and the standard deviations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "For clustering we covered two algorithms\n",
    "\n",
    "1) K-means : An iterative method to get 'K' clusters, initializing them randomly\n",
    "\n",
    "2) Hirarchical : An iterative method to get a dendogram of clustering with various numbers of cluster centers.\n",
    "\n",
    "### K-means Clustering\n",
    "\n",
    "We initialize $K$ cluster centers $\\{ c_1,c_2 ,... c_k\\}$for $K$-clusters randomly. All the data points are assigned a cluster index $D_i \\in \\{ 1,2,...,k\\}$, based on the closest cluster center to each point.\n",
    "\n",
    "Now, for each cluster, the cluster centers are re-evaluated as the mean of all the points in the center.\n",
    "\n",
    "$$\n",
    "c_i = mean(\\{ X_j | D_j = i \\})\n",
    "$$\n",
    "This process continues till convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "The dataset contains 1000  color images.Convert them to grayscale images. We need to cluster them into various $n$ clusters based on the similarity of their histograms. For each image, find the histogram with bin size 25 (last bin of 30;i.e.225-255;giving you 10 bins). Now treating each of these bins as seperate dimensions, find:\n",
    "\n",
    "a) Cluster Centers for $n = 5$ clusters, with $L_2$ distance criteria for measuring distance between a pair of images.\n",
    "\n",
    "b) **Bonus**: Use Earth Movers Distance in the above problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For this problem we will be using the 1000 test images of CIFAR-10 dataset.\n",
    "## Load the data from the following link\n",
    "# https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def l2_dist(a,b):\n",
    "\treturn np.linalg.norm(a-b)\n",
    "\t\n",
    "def find_nearest_center(cc,img,no_clusters):\n",
    "\tnear = 0;\n",
    "\ts_dist = l2_dist(img,cc[0])\n",
    "\t\n",
    "\tfor i in range(1,no_clusters):\n",
    "\t\tdist = l2_dist(img,cc[i])\n",
    "\t\tif dist<s_dist:\n",
    "\t\t\ts_dist=dist\n",
    "\t\t\tnear=i\n",
    "\treturn near\n",
    "\n",
    "def add_and_update_cc(img_no,center,img_10d):\n",
    "\tif cluster[center][0]!=img_no:\n",
    "\t\tcluster[center]=np.hstack((cluster[center],np.array([img_no])))\n",
    "\tno_elem = cluster[center].shape[0]\n",
    "\ttmp_cc = img_10d[cluster[center][0]]\n",
    "\t#print tmp_cc\n",
    "\tfor i in range(1,no_elem):\n",
    "\t\ttmp_cc=np.add(tmp_cc,img_10d[cluster[center][i]])\n",
    "\ttmp_cc = tmp_cc/no_elem\n",
    "\t#print \"updated center \"+str(center)+\" is \"\n",
    "\t#print tmp_cc\n",
    "\tcc[center]= tmp_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert it to grayscale values\n",
    "\n",
    "img_num=1000\n",
    "img_data = []\n",
    "gray_img = np.empty((0,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the histograms and get a 10-dimensional representation of each images.\n",
    "img_10d = np.empty((0,10))\n",
    "cluster = []\n",
    "no_clusters = 5\n",
    "cc = np.zeros((no_clusters,10))\n",
    "print cc\n",
    "for i in range(img_num):\n",
    "\timg = mpimg.imread('./test_images/image'+str(i)+'.png')\n",
    "    #i already extracted images and stored in the folder test_images which are the test images of CIFAR-10\n",
    "\t#print img[0][0]\n",
    "\timg_data.append(img)  \n",
    "\tgray = rgb2gray(img)\n",
    "\tmin1=gray.min();\n",
    "\tmax1=gray.max();\n",
    "\ty=((gray-min1)*255)/(max1-min1);\n",
    "\t#print y[0][0]\n",
    "\ty = y[np.newaxis,:,:]\n",
    "\t#print gray_img.shape\n",
    "\t#print y.shape\n",
    "\t#gray_img.append((y))\n",
    "\tgray_img = np.vstack([gray_img,y])\n",
    "\t\n",
    "\t#input()\n",
    "\t#mpimg.savefig('./test/image'+str(i)+'.png')    \n",
    "#plt.imshow(gray, cmap = plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use K-means to find  out the number of cluster centers.\n",
    "for i in range(img_num):\n",
    "\ta = plt.hist(gray_img[i].ravel(), bins =[0,25,50,75,100,125,150,175,200,225,255])\n",
    "\t#img_10d.append(a[0])\n",
    "\t#print img_10d[0]\n",
    "\ta=a[0]\n",
    "\t#print a\n",
    "\timg_10d = np.vstack([img_10d,a])\n",
    "#print img_10d.shape\n",
    "\n",
    "for x in range(no_clusters):\n",
    "\ttmp=random.randint(1,img_num-1)\n",
    "\tcc[x] = img_10d[tmp]\n",
    "\tcluster.append(np.array([tmp]))\n",
    "\n",
    "print cc\n",
    "\n",
    "\n",
    "for i in range(img_num):\n",
    "\t#print \"Updated cluster centers:\"\n",
    "\tnear_center =find_nearest_center(cc,img_10d[i],no_clusters)\n",
    "\tadd_and_update_cc(i,near_center,img_10d)\n",
    "\t#print cc\n",
    "\t#input()\n",
    "\t#print \"cluster centers\"\n",
    "#print cluster\n",
    "for i in xrange(no_clusters):\n",
    "\tprint \"elements of cluster \"+str(i)+'are :'\n",
    "\tprint cluster[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize cluster means to see what they look like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Useful references will be added shortly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
